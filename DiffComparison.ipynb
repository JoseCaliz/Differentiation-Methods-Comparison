{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install SymPy\n",
    "#Numpy precision is set to 16 or 18 depending on double or long double respectively, \n",
    "#  it is required to use mpmath to support more digits\n",
    "import sympy as smp\n",
    "import numpy as np\n",
    "from mpmath import *\n",
    "\n",
    "#Setting mpmath to have 50 digits of precision\n",
    "mp.dps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Norm\n",
    "\n",
    "In this example we shall see that by doing numerical differentiation, the truncation errors will be available. It is not recommended this procedure, unless the user do not know the right equation to model the data. For this example we'll see the derivative of the euclidean norm defined by:\n",
    "$$f(x) = \\sum_{i=1}^{n} x_{i}^{2}$$\n",
    "\n",
    "And we will calculate the gradients at:\n",
    "$$x_i = i \\quad \\text{for} \\quad i = 1, 2, \\dots, 10$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanNorm(X):\n",
    "    return np.sum(np.power(X,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the arrays, in this example a 10-dimension array\n",
    "   * $n$ = 10\n",
    "   * $e_1$' = $[1, 0, \\dots 0]$\n",
    "   * $h$ = $\\sqrt{2^{-54}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([a+1 for a in range(10)])\n",
    "e1 = np.array([0 if a>0 else 1 for a in range(10)])\n",
    "h = mpf((2**(-54))**(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first gradient component is defined by\n",
    "$$\\frac{1}{h}\\left[ f(x + h e_1) - f(x) \\right] = \\frac{\\partial}{\\partial x_1}f(x) + h = 2 x_1 + h = 2 + h$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.000000007450580596923828125\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the first gradient\n",
    "from operator import add\n",
    "a = [x*h for x in e1]\n",
    "b = list(map(add, a, X))\n",
    "diff1 = 1/h * (EuclideanNorm(b) - EuclideanNorm(X))\n",
    "print(diff1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the first gradient component is not equal to 2, this because of the truncated error. As a general rule, **derivatives must not incurr in truncation errors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
